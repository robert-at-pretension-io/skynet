2024-01-13 14:29:56,541 - INFO - __main__ - Skynet started
2024-01-13 14:29:56,541 - INFO - __main__ - Loading environment variables
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-13 14:29:56,541 - INFO - __main__ - Loaded functions from functions
2024-01-13 14:29:56,542 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-13 14:29:56,542 - INFO - functions.get_functions - file size: 2609
2024-01-13 14:29:56,543 - INFO - functions.get_functions - tree size: 16
2024-01-13 14:29:56,689 - INFO - functions.get_functions - Looking at function: get_links
2024-01-13 14:29:56,790 - INFO - functions.get_functions - Looking at function: process_command
2024-01-13 14:29:56,792 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-13 14:29:56,792 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-13 14:29:56,793 - INFO - functions.get_functions - file size: 1926
2024-01-13 14:29:56,793 - INFO - functions.get_functions - tree size: 5
2024-01-13 14:29:56,794 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-13 14:29:56,795 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-13 14:29:56,795 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-13 14:29:56,796 - INFO - functions.get_functions - file size: 2971
2024-01-13 14:29:56,797 - INFO - functions.get_functions - tree size: 10
2024-01-13 14:29:56,803 - INFO - functions.get_functions - Looking at function: read_file
2024-01-13 14:29:56,808 - INFO - functions.get_functions - Looking at function: write_file
2024-01-13 14:29:56,809 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-13 14:29:56,810 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-13 14:29:56,811 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-13 14:29:56,816 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-13 14:29:56,816 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-13 14:29:56,818 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-13 14:29:56,824 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-13 14:29:56,825 - INFO - functions.get_functions - file size: 0
2024-01-13 14:29:56,825 - INFO - functions.get_functions - tree size: 0
2024-01-13 14:29:56,825 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-13 14:29:56,826 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-13 14:29:56,826 - INFO - functions.get_functions - file size: 2135
2024-01-13 14:29:56,827 - INFO - functions.get_functions - tree size: 8
2024-01-13 14:29:56,827 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-13 14:29:56,828 - INFO - functions.get_functions - Looking at function: create_function
2024-01-13 14:29:56,829 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-13 14:29:56,830 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-13 14:29:56,832 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-13 14:29:56,840 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-13 14:29:56,841 - INFO - functions.get_functions - file size: 0
2024-01-13 14:29:56,841 - INFO - functions.get_functions - tree size: 0
2024-01-13 14:29:56,841 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-13 14:29:56,843 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-13 14:29:56,846 - INFO - functions.get_functions - file size: 5276
2024-01-13 14:29:56,850 - INFO - functions.get_functions - tree size: 9
2024-01-13 14:29:56,854 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-13 14:29:56,857 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-13 14:29:56,861 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-13 14:29:56,866 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-13 14:29:56,872 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-13 14:29:56,873 - INFO - functions.prompt_creation - Describing function
2024-01-13 14:29:56,879 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-13 14:29:56,917 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-13 14:29:57,053 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f8f9d3b2390>
2024-01-13 14:29:57,053 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f8f9dd06600> server_hostname='api.openai.com' timeout=5.0
2024-01-13 14:29:57,117 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f8f9db7ecd0>
2024-01-13 14:29:57,118 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-13 14:29:57,118 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-13 14:29:57,118 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-13 14:29:57,119 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-13 14:29:57,119 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-13 14:29:59,062 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 13 Jan 2024 19:29:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1832'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79370'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'472ms'), (b'x-request-id', b'7fa1197b44352f0c7b3406b7f62617d0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=IXURbJxAgM8SVdq6AuN2P5xzRW8pph50VAcI_7DPaec-1705174199-1-AQ6eEn3+TnBBa7Bb0KRmFIRySzJflux59DpiHYpWqdCgHJC8OxXDuTwhcVbNf8jnvLtqqF39WVs4yxY2jzwVMOg=; path=/; expires=Sat, 13-Jan-24 19:59:59 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=pKqUAUfvVxrme34W1Y2v6q9TQ.eRooValIW8LL7mqPY-1705174199041-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84500d0c2eb97bb1-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-13 14:29:59,063 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-13 14:29:59,064 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-13 14:29:59,065 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-13 14:29:59,065 - DEBUG - httpcore.http11 - response_closed.started
2024-01-13 14:29:59,065 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-13 14:29:59,066 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-13 14:29:59,069 - INFO - functions.prompt_creation - Describing function
2024-01-13 14:29:59,080 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_functions(functions):\n    logger.info('Available functions:')\n    for func_name, func_info in functions.items():\n        arg_types = func_info['arg_types']\n        logger.info(f'Function: {func_name}, Argument Types: {arg_types}')\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-13 14:29:59,082 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-13 14:29:59,083 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-13 14:29:59,083 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-13 14:29:59,084 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-13 14:29:59,084 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-13 14:30:01,213 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 13 Jan 2024 19:30:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1987'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79266'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'550ms'), (b'x-request-id', b'871161f984fbfa4b6370f8f7ae47ff72'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84500d18684d7bb1-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-13 14:30:01,214 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-13 14:30:01,215 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-13 14:30:01,215 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-13 14:30:01,216 - DEBUG - httpcore.http11 - response_closed.started
2024-01-13 14:30:01,216 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-13 14:30:01,216 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-13 14:30:01,218 - INFO - functions.prompt_creation - Describing function
2024-01-13 14:30:01,229 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_functions(functions):\n    logger.info('Available functions:')\n    for func_name, func_info in functions.items():\n        arg_types = func_info['arg_types']\n        logger.info(f'Function: {func_name}, Argument Types: {arg_types}')\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_argument_values(arg_types):\n    args = []\n    for arg_name, arg_type in arg_types.items():\n        while True:\n            user_input = input(f'Enter value for {arg_name} ({arg_type}): ')\n            try:\n                if arg_type == int:\n                    converted_value = int(user_input)\n                elif arg_type == float:\n                    converted_value = float(user_input)\n                elif arg_type == bool:\n                    converted_value = user_input.lower() in ['true', '1', 'yes']\n                elif arg_type == str:\n                    converted_value = user_input\n                else:\n                    converted_value = eval(user_input)\n                if not isinstance(converted_value, arg_type) and arg_type != inspect._empty:\n                    raise TypeError(f'Incorrect type for {arg_name}, expected {arg_type}')\n                break\n            except ValueError as e:\n                logger.error(f'Invalid input: {e}')\n            except TypeError as e:\n                logger.error(e)\n            except Exception as e:\n                logger.error(f'Error processing input: {e}')\n        args.append(converted_value)\n    return args\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-13 14:30:01,230 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-13 14:30:02,748 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 13 Jan 2024 19:30:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1174'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78923'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'807ms'), (b'x-request-id', b'f21e679a51c0ea669e3063e7cb2f2599'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84500d25da217bb1-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-13 14:30:02,749 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-13 14:30:02,749 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-13 14:30:02,751 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-13 14:30:02,751 - DEBUG - httpcore.http11 - response_closed.started
2024-01-13 14:30:02,751 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-13 14:30:02,751 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-13 14:30:02,754 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-13 14:30:02,903 - DEBUG - httpcore.connection - close.started
2024-01-13 14:30:02,904 - DEBUG - httpcore.connection - close.complete
2024-01-14 11:11:53,924 - INFO - __main__ - Skynet started
2024-01-14 11:11:53,924 - INFO - __main__ - Loading environment variables
2024-01-14 11:11:53,924 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 11:11:53,924 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 11:11:53,924 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 11:11:53,924 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 11:11:53,925 - INFO - __main__ - Loaded functions from functions
2024-01-14 11:11:53,925 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 11:11:53,925 - INFO - functions.get_functions - file size: 2609
2024-01-14 11:11:53,926 - INFO - functions.get_functions - tree size: 16
2024-01-14 11:11:54,101 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 11:11:54,230 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 11:11:54,232 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 11:11:54,232 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 11:11:54,232 - INFO - functions.get_functions - file size: 1926
2024-01-14 11:11:54,233 - INFO - functions.get_functions - tree size: 5
2024-01-14 11:11:54,233 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 11:11:54,234 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 11:11:54,234 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 11:11:54,234 - INFO - functions.get_functions - file size: 2971
2024-01-14 11:11:54,236 - INFO - functions.get_functions - tree size: 10
2024-01-14 11:11:54,236 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 11:11:54,237 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 11:11:54,237 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 11:11:54,238 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 11:11:54,238 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 11:11:54,239 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 11:11:54,240 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 11:11:54,241 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 11:11:54,241 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 11:11:54,242 - INFO - functions.get_functions - file size: 0
2024-01-14 11:11:54,242 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:11:54,242 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 11:11:54,242 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 11:11:54,242 - INFO - functions.get_functions - file size: 2135
2024-01-14 11:11:54,243 - INFO - functions.get_functions - tree size: 8
2024-01-14 11:11:54,243 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 11:11:54,306 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 11:11:54,307 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 11:11:54,308 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 11:11:54,309 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-14 11:11:54,309 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 11:11:54,310 - INFO - functions.get_functions - file size: 0
2024-01-14 11:11:54,310 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:11:54,310 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 11:11:54,310 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 11:11:54,310 - INFO - functions.get_functions - file size: 5276
2024-01-14 11:11:54,312 - INFO - functions.get_functions - tree size: 9
2024-01-14 11:11:54,312 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 11:11:54,314 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 11:11:54,315 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 11:11:54,317 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 11:11:54,318 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 11:11:54,318 - ERROR - __main__ - Error loading existing function info from serialized_function_info.json
2024-01-14 11:13:25,327 - INFO - __main__ - Skynet started
2024-01-14 11:13:25,328 - INFO - __main__ - Loading environment variables
2024-01-14 11:13:25,328 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 11:13:25,328 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 11:13:25,328 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 11:13:25,328 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 11:13:25,328 - INFO - __main__ - Loaded functions from functions
2024-01-14 11:13:25,328 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 11:13:25,328 - INFO - functions.get_functions - file size: 2609
2024-01-14 11:13:25,330 - INFO - functions.get_functions - tree size: 16
2024-01-14 11:13:25,483 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 11:13:25,579 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 11:13:25,581 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 11:13:25,581 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 11:13:25,581 - INFO - functions.get_functions - file size: 1926
2024-01-14 11:13:25,582 - INFO - functions.get_functions - tree size: 5
2024-01-14 11:13:25,582 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 11:13:25,583 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 11:13:25,584 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 11:13:25,584 - INFO - functions.get_functions - file size: 2971
2024-01-14 11:13:25,585 - INFO - functions.get_functions - tree size: 10
2024-01-14 11:13:25,586 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 11:13:25,587 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 11:13:25,589 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 11:13:25,590 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 11:13:25,591 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 11:13:25,595 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 11:13:25,596 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 11:13:25,598 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 11:13:25,600 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 11:13:25,600 - INFO - functions.get_functions - file size: 0
2024-01-14 11:13:25,601 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:13:25,601 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 11:13:25,602 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 11:13:25,602 - INFO - functions.get_functions - file size: 2135
2024-01-14 11:13:25,603 - INFO - functions.get_functions - tree size: 8
2024-01-14 11:13:25,605 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 11:13:25,607 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 11:13:25,609 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 11:13:25,611 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 11:13:25,613 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-14 11:13:25,614 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 11:13:25,615 - INFO - functions.get_functions - file size: 0
2024-01-14 11:13:25,616 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:13:25,616 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 11:13:25,617 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 11:13:25,618 - INFO - functions.get_functions - file size: 5276
2024-01-14 11:13:25,621 - INFO - functions.get_functions - tree size: 9
2024-01-14 11:13:25,622 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 11:13:25,625 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 11:13:25,626 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 11:13:25,631 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 11:13:25,633 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 11:13:25,635 - ERROR - __main__ - Error creating file serialized_function_info.json
2024-01-14 11:29:21,576 - INFO - __main__ - Skynet started
2024-01-14 11:29:21,579 - INFO - __main__ - Loading environment variables
2024-01-14 11:29:21,579 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 11:29:21,579 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 11:29:21,579 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 11:29:21,580 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 11:29:21,580 - INFO - __main__ - Loaded functions from functions
2024-01-14 11:29:21,580 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 11:29:21,580 - INFO - functions.get_functions - file size: 2609
2024-01-14 11:29:21,581 - INFO - functions.get_functions - tree size: 16
2024-01-14 11:29:21,815 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 11:29:21,994 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 11:29:21,995 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 11:29:21,995 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 11:29:21,996 - INFO - functions.get_functions - file size: 1926
2024-01-14 11:29:21,996 - INFO - functions.get_functions - tree size: 5
2024-01-14 11:29:21,996 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 11:29:21,998 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 11:29:21,998 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 11:29:21,998 - INFO - functions.get_functions - file size: 2971
2024-01-14 11:29:21,999 - INFO - functions.get_functions - tree size: 10
2024-01-14 11:29:21,999 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 11:29:22,000 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 11:29:22,000 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 11:29:22,001 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 11:29:22,001 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 11:29:22,002 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 11:29:22,002 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 11:29:22,004 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 11:29:22,004 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 11:29:22,004 - INFO - functions.get_functions - file size: 0
2024-01-14 11:29:22,004 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:29:22,004 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 11:29:22,004 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 11:29:22,005 - INFO - functions.get_functions - file size: 2135
2024-01-14 11:29:22,005 - INFO - functions.get_functions - tree size: 8
2024-01-14 11:29:22,005 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 11:29:22,007 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 11:29:22,007 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 11:29:22,008 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 11:29:22,013 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-14 11:29:22,013 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 11:29:22,014 - INFO - functions.get_functions - file size: 0
2024-01-14 11:29:22,014 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:29:22,014 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 11:29:22,014 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 11:29:22,014 - INFO - functions.get_functions - file size: 5276
2024-01-14 11:29:22,016 - INFO - functions.get_functions - tree size: 9
2024-01-14 11:29:22,016 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 11:29:22,018 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 11:29:22,018 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 11:29:22,021 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 11:29:22,021 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 11:29:22,021 - ERROR - __main__ - Error loading existing function info from serialized_function_info.json
2024-01-14 11:30:59,896 - INFO - __main__ - Skynet started
2024-01-14 11:30:59,898 - INFO - __main__ - Loading environment variables
2024-01-14 11:30:59,898 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 11:30:59,899 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 11:30:59,899 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 11:30:59,900 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 11:30:59,900 - INFO - __main__ - Loaded functions from functions
2024-01-14 11:30:59,901 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 11:30:59,902 - INFO - functions.get_functions - file size: 2609
2024-01-14 11:30:59,903 - INFO - functions.get_functions - tree size: 16
2024-01-14 11:31:00,074 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 11:31:00,240 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 11:31:00,241 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 11:31:00,248 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 11:31:00,249 - INFO - functions.get_functions - file size: 1926
2024-01-14 11:31:00,250 - INFO - functions.get_functions - tree size: 5
2024-01-14 11:31:00,250 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 11:31:00,255 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 11:31:00,255 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 11:31:00,256 - INFO - functions.get_functions - file size: 2971
2024-01-14 11:31:00,257 - INFO - functions.get_functions - tree size: 10
2024-01-14 11:31:00,265 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 11:31:00,267 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 11:31:00,273 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 11:31:00,274 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 11:31:00,279 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 11:31:00,280 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 11:31:00,281 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 11:31:00,283 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 11:31:00,297 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 11:31:00,297 - INFO - functions.get_functions - file size: 0
2024-01-14 11:31:00,297 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:31:00,298 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 11:31:00,298 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 11:31:00,298 - INFO - functions.get_functions - file size: 2135
2024-01-14 11:31:00,299 - INFO - functions.get_functions - tree size: 8
2024-01-14 11:31:00,299 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 11:31:00,301 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 11:31:00,302 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 11:31:00,302 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 11:31:00,303 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-14 11:31:00,309 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 11:31:00,309 - INFO - functions.get_functions - file size: 0
2024-01-14 11:31:00,309 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:31:00,310 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 11:31:00,310 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 11:31:00,310 - INFO - functions.get_functions - file size: 5276
2024-01-14 11:31:00,312 - INFO - functions.get_functions - tree size: 9
2024-01-14 11:31:00,320 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 11:31:00,322 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 11:31:00,329 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 11:31:00,332 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 11:31:00,333 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 11:31:00,335 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:31:00,354 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:31:00,442 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-14 11:31:00,565 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f50ca8472d0>
2024-01-14 11:31:00,567 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f50cae46690> server_hostname='api.openai.com' timeout=5.0
2024-01-14 11:31:00,862 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f50cacc6310>
2024-01-14 11:31:00,862 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:31:00,863 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:31:00,863 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:31:00,863 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:31:00,863 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:31:03,538 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:31:03 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'2532'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79370'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'472ms'), (b'x-request-id', b'8572ba8735df2b9fe7a4ca842575525c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=hJVq9k1eTddKJ3Erby9lHBklyvIg61LcOOWqu4mqEfI-1705249863-1-ATtgHJMontWTprnUe+LNZuHYahL3qsAyTVz7ThpWjtH29Hi6uMB+XB8v+dgoosGfWsvYzRluIuemNivEcOxUSkI=; path=/; expires=Sun, 14-Jan-24 17:01:03 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=fpEFH5Aq.ka4s_W9Tgdo3QD_F1VuSlU6l610xJS_EXQ-1705249863506-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8457444e79477ced-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:31:03,539 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:31:03,540 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:31:03,541 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:31:03,541 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:31:03,541 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:31:03,541 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:31:03,576 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:31:03,584 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_functions(functions):\n    logger.info('Available functions:')\n    for func_name, func_info in functions.items():\n        arg_types = func_info['arg_types']\n        logger.info(f'Function: {func_name}, Argument Types: {arg_types}')\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:31:03,590 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:31:03,591 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:31:03,591 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:31:03,592 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:31:03,593 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:31:04,901 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:31:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'830'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79266'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'550ms'), (b'x-request-id', b'3be9c92ed30557c5cf18f2780f529916'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8457445f7d637ced-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:31:04,903 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:31:04,904 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:31:04,905 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:31:04,906 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:31:04,906 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:31:04,907 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:31:04,917 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:31:04,926 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_functions(functions):\n    logger.info('Available functions:')\n    for func_name, func_info in functions.items():\n        arg_types = func_info['arg_types']\n        logger.info(f'Function: {func_name}, Argument Types: {arg_types}')\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_argument_values(arg_types):\n    args = []\n    for arg_name, arg_type in arg_types.items():\n        while True:\n            user_input = input(f'Enter value for {arg_name} ({arg_type}): ')\n            try:\n                if arg_type == int:\n                    converted_value = int(user_input)\n                elif arg_type == float:\n                    converted_value = float(user_input)\n                elif arg_type == bool:\n                    converted_value = user_input.lower() in ['true', '1', 'yes']\n                elif arg_type == str:\n                    converted_value = user_input\n                else:\n                    converted_value = eval(user_input)\n                if not isinstance(converted_value, arg_type) and arg_type != inspect._empty:\n                    raise TypeError(f'Incorrect type for {arg_name}, expected {arg_type}')\n                break\n            except ValueError as e:\n                logger.error(f'Invalid input: {e}')\n            except TypeError as e:\n                logger.error(e)\n            except Exception as e:\n                logger.error(f'Error processing input: {e}')\n        args.append(converted_value)\n    return args\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:31:04,928 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:31:04,929 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:31:04,929 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:31:04,930 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:31:04,930 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:31:07,063 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:31:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1974'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78923'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'807ms'), (b'x-request-id', b'dc0d02383447c16537d20379d2c19f5b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84574467eed37ced-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:31:07,063 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:31:07,063 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:31:07,065 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:31:07,066 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:31:07,067 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:31:07,068 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:31:07,073 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 11:31:07,224 - DEBUG - httpcore.connection - close.started
2024-01-14 11:31:07,224 - DEBUG - httpcore.connection - close.complete
2024-01-14 11:31:27,391 - INFO - __main__ - Skynet started
2024-01-14 11:31:27,391 - INFO - __main__ - Loading environment variables
2024-01-14 11:31:27,391 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 11:31:27,391 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 11:31:27,392 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 11:31:27,392 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 11:31:27,392 - INFO - __main__ - Loaded functions from functions
2024-01-14 11:31:27,392 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 11:31:27,393 - INFO - functions.get_functions - file size: 2609
2024-01-14 11:31:27,394 - INFO - functions.get_functions - tree size: 16
2024-01-14 11:31:27,534 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 11:31:27,621 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 11:31:27,622 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 11:31:27,622 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 11:31:27,623 - INFO - functions.get_functions - file size: 1926
2024-01-14 11:31:27,623 - INFO - functions.get_functions - tree size: 5
2024-01-14 11:31:27,623 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 11:31:27,625 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 11:31:27,625 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 11:31:27,625 - INFO - functions.get_functions - file size: 2971
2024-01-14 11:31:27,626 - INFO - functions.get_functions - tree size: 10
2024-01-14 11:31:27,626 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 11:31:27,627 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 11:31:27,628 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 11:31:27,628 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 11:31:27,629 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 11:31:27,630 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 11:31:27,630 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 11:31:27,632 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 11:31:27,632 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 11:31:27,632 - INFO - functions.get_functions - file size: 0
2024-01-14 11:31:27,633 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:31:27,633 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 11:31:27,633 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 11:31:27,633 - INFO - functions.get_functions - file size: 2135
2024-01-14 11:31:27,634 - INFO - functions.get_functions - tree size: 8
2024-01-14 11:31:27,634 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 11:31:27,635 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 11:31:27,636 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 11:31:27,637 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 11:31:27,638 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-14 11:31:27,638 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 11:31:27,638 - INFO - functions.get_functions - file size: 0
2024-01-14 11:31:27,638 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:31:27,639 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 11:31:27,639 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 11:31:27,639 - INFO - functions.get_functions - file size: 5276
2024-01-14 11:31:27,641 - INFO - functions.get_functions - tree size: 9
2024-01-14 11:31:27,641 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 11:31:27,643 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 11:31:27,644 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 11:31:27,646 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 11:31:27,647 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 11:31:27,647 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 11:50:01,581 - INFO - __main__ - Skynet started
2024-01-14 11:50:01,582 - INFO - __main__ - Loading environment variables
2024-01-14 11:50:01,582 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 11:50:01,583 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 11:50:01,583 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 11:50:01,583 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 11:50:01,583 - INFO - __main__ - Loaded functions from functions
2024-01-14 11:50:01,583 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 11:50:01,583 - INFO - functions.get_functions - file size: 2609
2024-01-14 11:50:01,584 - INFO - functions.get_functions - tree size: 16
2024-01-14 11:50:01,815 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 11:50:01,921 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 11:50:01,923 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 11:50:01,923 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 11:50:01,923 - INFO - functions.get_functions - file size: 1926
2024-01-14 11:50:01,924 - INFO - functions.get_functions - tree size: 5
2024-01-14 11:50:01,924 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 11:50:01,925 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 11:50:01,926 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 11:50:01,926 - INFO - functions.get_functions - file size: 2971
2024-01-14 11:50:01,927 - INFO - functions.get_functions - tree size: 10
2024-01-14 11:50:01,927 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 11:50:01,928 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 11:50:01,929 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 11:50:01,929 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 11:50:01,930 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 11:50:01,931 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 11:50:01,932 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 11:50:01,933 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 11:50:01,934 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 11:50:01,934 - INFO - functions.get_functions - file size: 0
2024-01-14 11:50:01,934 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:50:01,934 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 11:50:01,934 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 11:50:01,934 - INFO - functions.get_functions - file size: 2135
2024-01-14 11:50:01,935 - INFO - functions.get_functions - tree size: 8
2024-01-14 11:50:01,935 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 11:50:01,937 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 11:50:01,937 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 11:50:01,938 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 11:50:01,939 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-14 11:50:01,939 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 11:50:01,939 - INFO - functions.get_functions - file size: 0
2024-01-14 11:50:01,940 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:50:01,940 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 11:50:01,940 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 11:50:01,940 - INFO - functions.get_functions - file size: 5276
2024-01-14 11:50:01,942 - INFO - functions.get_functions - tree size: 9
2024-01-14 11:50:01,942 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 11:50:01,945 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 11:50:01,945 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 11:50:01,952 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 11:50:01,952 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 11:50:01,954 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 11:50:01,957 - INFO - __main__ - The following 3 function(s) were loaded: ['load_functions_from_file', 'list_functions', 'get_argument_values']
2024-01-14 11:53:30,107 - INFO - __main__ - Skynet started
2024-01-14 11:53:30,108 - INFO - __main__ - Loading environment variables
2024-01-14 11:53:30,108 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 11:53:30,108 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 11:53:30,108 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 11:53:30,108 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 11:53:30,108 - INFO - __main__ - Loaded functions from functions
2024-01-14 11:53:30,109 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 11:53:30,109 - INFO - functions.get_functions - file size: 2609
2024-01-14 11:53:30,110 - INFO - functions.get_functions - tree size: 16
2024-01-14 11:53:30,254 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 11:53:30,390 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 11:53:30,391 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 11:53:30,393 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 11:53:30,393 - INFO - functions.get_functions - file size: 1926
2024-01-14 11:53:30,394 - INFO - functions.get_functions - tree size: 5
2024-01-14 11:53:30,394 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 11:53:30,396 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 11:53:30,396 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 11:53:30,397 - INFO - functions.get_functions - file size: 2971
2024-01-14 11:53:30,398 - INFO - functions.get_functions - tree size: 10
2024-01-14 11:53:30,398 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 11:53:30,399 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 11:53:30,400 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 11:53:30,401 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 11:53:30,404 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 11:53:30,406 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 11:53:30,406 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 11:53:30,414 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 11:53:30,414 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 11:53:30,415 - INFO - functions.get_functions - file size: 0
2024-01-14 11:53:30,415 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:53:30,415 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 11:53:30,415 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 11:53:30,415 - INFO - functions.get_functions - file size: 2135
2024-01-14 11:53:30,416 - INFO - functions.get_functions - tree size: 8
2024-01-14 11:53:30,416 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 11:53:30,417 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 11:53:30,418 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 11:53:30,418 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 11:53:30,419 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-14 11:53:30,419 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 11:53:30,419 - INFO - functions.get_functions - file size: 0
2024-01-14 11:53:30,420 - INFO - functions.get_functions - tree size: 0
2024-01-14 11:53:30,420 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 11:53:30,420 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 11:53:30,420 - INFO - functions.get_functions - file size: 5276
2024-01-14 11:53:30,422 - INFO - functions.get_functions - tree size: 9
2024-01-14 11:53:30,422 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 11:53:30,424 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 11:53:30,425 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 11:53:30,427 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 11:53:30,428 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 11:53:30,428 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:30,463 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:30,546 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-14 11:53:30,857 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f847392b990>
2024-01-14 11:53:30,858 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f847428e720> server_hostname='api.openai.com' timeout=5.0
2024-01-14 11:53:31,088 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f8473c98050>
2024-01-14 11:53:31,089 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:31,089 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:31,090 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:31,090 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:31,090 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:33,057 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1657'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79891'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'81ms'), (b'x-request-id', b'503e5ca08253064767c5efdc0462fdec'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Vjux58Fwd5S0xww_Co37P_4dhhiPZ9Gk5EhZcbalA34-1705251212-1-AXDg0Ztig/CKebgUMHZ4+gCyNTl2ZILMcG78OlpKEo4dsshe5mRBHPMwpfZTZRr1Pk/JxZ9GovmAhCaS8NDdMxg=; path=/; expires=Sun, 14-Jan-24 17:23:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=xQ.NvK7TMOMaXU0_ec0Pq6yjRfzsSWYWAizC6EGfie4-1705251212916-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84576545b9b44344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:33,059 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:33,060 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:33,061 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:33,062 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:33,062 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:33,062 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:33,067 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:33,075 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:33,077 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:33,079 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:33,079 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:33,080 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:33,082 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:35,250 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1963'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79759'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'180ms'), (b'x-request-id', b'1bc6458a4b929791f304a7389ddd358e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765522f274344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:35,251 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:35,252 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:35,253 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:35,254 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:35,255 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:35,256 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:35,262 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:35,273 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:35,275 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:35,276 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:35,276 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:35,277 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:35,277 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:37,553 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1762'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79477'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'392ms'), (b'x-request-id', b'086fa87dc6ffd9fe296ba2de9ccea59e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8457655fedb24344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:37,554 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:37,554 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:37,555 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:37,555 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:37,555 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:37,556 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:37,558 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:37,567 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:37,575 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:37,577 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:37,579 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:37,580 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:37,580 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:39,432 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1396'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79341'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'494ms'), (b'x-request-id', b'fbab2a93fdc5177df8066ffada3a838c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8457656e4db44344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:39,433 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:39,433 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:39,434 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:39,434 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:39,434 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:39,434 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:39,436 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:39,445 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:39,447 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:39,447 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:39,447 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:39,448 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:39,448 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:41,925 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'2305'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79175'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'618ms'), (b'x-request-id', b'b086782ed69e389c33adb9d188a35102'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84576579fae74344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:41,926 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:41,926 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:41,927 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:41,927 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:41,927 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:41,927 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:41,929 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:41,940 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:41,941 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:41,942 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:41,942 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:41,942 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:41,942 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:42,741 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'389'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79098'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'676ms'), (b'x-request-id', b'6c45e465903ed84552c147efe9bda9bc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765898f9f4344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:42,741 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:42,742 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:42,742 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:42,742 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:42,742 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:42,742 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:42,744 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:42,791 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:42,805 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:42,808 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:42,812 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:42,815 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:42,821 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:45,390 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'2227'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78978'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'766ms'), (b'x-request-id', b'e973b192ad280341067d6b644cefd210'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8457658f9e194344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:45,391 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:45,391 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:45,393 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:45,394 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:45,395 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:45,395 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:45,398 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:45,413 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_files_in_directory(directory=\'\'):\n    """Lists all files in a given directory.\n\n    Args:\n    - directory: The path to the directory.\n\n    Returns:\n    - A list of file names in the directory.\n    """\n    try:\n        logger.info(f\'Listing files in directory {directory}\')\n        return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n    except FileNotFoundError:\n        print(f\'Directory {directory} not found.\')\n        return []\n    except OSError as e:\n        print(f\'Error accessing directory {directory}: {e}\')\n        return []\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:45,415 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:45,416 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:45,417 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:45,418 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:45,424 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:47,762 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'2152'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78786'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'910ms'), (b'x-request-id', b'59eb800fb805fda1b7b1a6d4d74ed90d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8457659f68ef4344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:47,763 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:47,792 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:47,793 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:47,797 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:47,799 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:47,800 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:47,803 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:47,826 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_files_in_directory(directory=\'\'):\n    """Lists all files in a given directory.\n\n    Args:\n    - directory: The path to the directory.\n\n    Returns:\n    - A list of file names in the directory.\n    """\n    try:\n        logger.info(f\'Listing files in directory {directory}\')\n        return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n    except FileNotFoundError:\n        print(f\'Directory {directory} not found.\')\n        return []\n    except OSError as e:\n        print(f\'Error accessing directory {directory}: {e}\')\n        return []\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_current_working_directory():\n    """Returns the current working directory."""\n    logger.info(\'Getting current working directory\')\n    return os.getcwd()\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:47,829 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:47,830 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:47,831 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:47,831 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:47,832 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:49,466 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1313'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78702'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'973ms'), (b'x-request-id', b'4e586f026d1cbbd58196528d43be4545'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765ae596b4344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:49,467 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:49,467 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:49,468 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:49,469 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:49,469 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:49,470 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:49,482 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:49,507 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_files_in_directory(directory=\'\'):\n    """Lists all files in a given directory.\n\n    Args:\n    - directory: The path to the directory.\n\n    Returns:\n    - A list of file names in the directory.\n    """\n    try:\n        logger.info(f\'Listing files in directory {directory}\')\n        return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n    except FileNotFoundError:\n        print(f\'Directory {directory} not found.\')\n        return []\n    except OSError as e:\n        print(f\'Error accessing directory {directory}: {e}\')\n        return []\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_current_working_directory():\n    """Returns the current working directory."""\n    logger.info(\'Getting current working directory\')\n    return os.getcwd()\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def set_env_variables_with_defaults(env_vars_defaults):\n    """\n    Sets environment variables to default values if they are not already set.\n\n    Args:\n    - env_vars_defaults: A dictionary where keys are environment variable names \n                         and values are the default values for these variables.\n    """\n    for var, default in env_vars_defaults.items():\n        if var not in os.environ:\n            if default != \'error\':\n                logger.info(f\'Setting environment variable {var} to default value {default}\')\n                os.environ[var] = default\n            else:\n                logger.error(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n                raise ValueError(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:49,509 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:49,510 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:49,510 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:49,511 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:49,512 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:51,346 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1161'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78443'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1.167s'), (b'x-request-id', b'd9e58d32ae8fbe15e5b84f3cb1cf14e9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765b8feeb4344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:51,347 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:51,348 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:51,349 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:51,349 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:51,349 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:51,349 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:51,351 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:51,383 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_files_in_directory(directory=\'\'):\n    """Lists all files in a given directory.\n\n    Args:\n    - directory: The path to the directory.\n\n    Returns:\n    - A list of file names in the directory.\n    """\n    try:\n        logger.info(f\'Listing files in directory {directory}\')\n        return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n    except FileNotFoundError:\n        print(f\'Directory {directory} not found.\')\n        return []\n    except OSError as e:\n        print(f\'Error accessing directory {directory}: {e}\')\n        return []\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_current_working_directory():\n    """Returns the current working directory."""\n    logger.info(\'Getting current working directory\')\n    return os.getcwd()\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def set_env_variables_with_defaults(env_vars_defaults):\n    """\n    Sets environment variables to default values if they are not already set.\n\n    Args:\n    - env_vars_defaults: A dictionary where keys are environment variable names \n                         and values are the default values for these variables.\n    """\n    for var, default in env_vars_defaults.items():\n        if var not in os.environ:\n            if default != \'error\':\n                logger.info(f\'Setting environment variable {var} to default value {default}\')\n                os.environ[var] = default\n            else:\n                logger.error(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n                raise ValueError(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n    \n    Description:'}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_function_options(functions: [FunctionInfo]) -> str:\n    function_options = ''\n    for function in functions:\n        function_options += f'{function.function_name} : {function.description}\\n'\n    return function_options\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:51,386 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:51,387 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:51,388 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:51,389 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:51,389 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:53,299 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1669'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78341'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1.244s'), (b'x-request-id', b'84300a163449c08d2c97019b383293e8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765c48e344344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:53,300 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:53,300 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:53,301 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:53,301 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:53,301 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:53,301 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:53,303 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:53,321 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_files_in_directory(directory=\'\'):\n    """Lists all files in a given directory.\n\n    Args:\n    - directory: The path to the directory.\n\n    Returns:\n    - A list of file names in the directory.\n    """\n    try:\n        logger.info(f\'Listing files in directory {directory}\')\n        return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n    except FileNotFoundError:\n        print(f\'Directory {directory} not found.\')\n        return []\n    except OSError as e:\n        print(f\'Error accessing directory {directory}: {e}\')\n        return []\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_current_working_directory():\n    """Returns the current working directory."""\n    logger.info(\'Getting current working directory\')\n    return os.getcwd()\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def set_env_variables_with_defaults(env_vars_defaults):\n    """\n    Sets environment variables to default values if they are not already set.\n\n    Args:\n    - env_vars_defaults: A dictionary where keys are environment variable names \n                         and values are the default values for these variables.\n    """\n    for var, default in env_vars_defaults.items():\n        if var not in os.environ:\n            if default != \'error\':\n                logger.info(f\'Setting environment variable {var} to default value {default}\')\n                os.environ[var] = default\n            else:\n                logger.error(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n                raise ValueError(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n    \n    Description:'}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_function_options(functions: [FunctionInfo]) -> str:\n    function_options = ''\n    for function in functions:\n        function_options += f'{function.function_name} : {function.description}\\n'\n    return function_options\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def create_function(function_objective: str, language: str) -> object:\n    prompt = f'Create a function that accomplishes the following: {function_objective}\\n\\n    Using the {language} programming language.\\n\\n    it should return a json object with the following properties:\\n\\n    required_libraries: a list of strings\\n\\n    source_code: a string\\n\\n    The source code should ONLY be the function definition, including the function definition and the function body.\\n    '\n    try:\n        return return_gpt_response(prompt=prompt, return_json_oject=True)\n    except Exception as e:\n        raise ValueError(f'Error creating function: {e}')\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:53,323 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:53,323 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:53,324 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:53,324 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:53,324 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:55,215 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1719'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78136'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1.398s'), (b'x-request-id', b'3ec4196ee95cf47639e8973cad1f721d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765d0acc04344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:55,216 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:55,216 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:55,216 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:55,216 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:55,216 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:55,217 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:55,219 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:55,239 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_files_in_directory(directory=\'\'):\n    """Lists all files in a given directory.\n\n    Args:\n    - directory: The path to the directory.\n\n    Returns:\n    - A list of file names in the directory.\n    """\n    try:\n        logger.info(f\'Listing files in directory {directory}\')\n        return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n    except FileNotFoundError:\n        print(f\'Directory {directory} not found.\')\n        return []\n    except OSError as e:\n        print(f\'Error accessing directory {directory}: {e}\')\n        return []\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_current_working_directory():\n    """Returns the current working directory."""\n    logger.info(\'Getting current working directory\')\n    return os.getcwd()\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def set_env_variables_with_defaults(env_vars_defaults):\n    """\n    Sets environment variables to default values if they are not already set.\n\n    Args:\n    - env_vars_defaults: A dictionary where keys are environment variable names \n                         and values are the default values for these variables.\n    """\n    for var, default in env_vars_defaults.items():\n        if var not in os.environ:\n            if default != \'error\':\n                logger.info(f\'Setting environment variable {var} to default value {default}\')\n                os.environ[var] = default\n            else:\n                logger.error(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n                raise ValueError(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n    \n    Description:'}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_function_options(functions: [FunctionInfo]) -> str:\n    function_options = ''\n    for function in functions:\n        function_options += f'{function.function_name} : {function.description}\\n'\n    return function_options\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def create_function(function_objective: str, language: str) -> object:\n    prompt = f'Create a function that accomplishes the following: {function_objective}\\n\\n    Using the {language} programming language.\\n\\n    it should return a json object with the following properties:\\n\\n    required_libraries: a list of strings\\n\\n    source_code: a string\\n\\n    The source code should ONLY be the function definition, including the function definition and the function body.\\n    '\n    try:\n        return return_gpt_response(prompt=prompt, return_json_oject=True)\n    except Exception as e:\n        raise ValueError(f'Error creating function: {e}')\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def create_step_list(goal: str) -> object:\n    prompt = f'Create a step list that accomplishes the following: {goal}\\n\\n    The json object returned should have the following properties:\\n    step_list: a list of strings describing the steps to accomplish the goal\\n    verification: a string describing how to verify that the goal has been accomplished\\n    '\n    try:\n        return return_gpt_response(prompt=prompt, return_json_oject=True)\n    except Exception as e:\n        raise ValueError(f'Error creating step list: {e}')\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:55,241 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:55,241 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:55,241 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:55,242 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:55,242 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:57,296 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1211'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'77959'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1.53s'), (b'x-request-id', b'f2f13a2db937ed039c888482ceb29a7d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765dc99834344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:57,297 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:57,297 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:57,298 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:57,298 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:57,299 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:57,299 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:57,301 - INFO - functions.prompt_creation - Describing function
2024-01-14 11:53:57,327 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_links(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    links = [a.get('href') for a in soup.find_all('a', href=True)]\n    return links\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def process_command(command):\n    tokens = command.split()\n    if tokens[0] == 'get_links':\n        try:\n            links = get_links(tokens[1])\n            return links\n        except ValueError:\n            return 'Error: Please provide valid numbers.'\n    elif tokens[0] == 'exit':\n        return None\n    else:\n        return 'Unknown command'\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_gpt_response(message_log=[], prompt='', model='', return_json_oject=False):\n    if message_log == [] and prompt == '':\n        raise ValueError('Both message_log and prompt cannot be empty when calling return_chat_response.')\n    if model == '':\n        model = os.environ['DEFAULT_GPT_MODEL']\n    if prompt != '':\n        message_log.append({'role': 'user', 'content': prompt})\n    if return_json_oject:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log, response_format={'type': 'json_object'})\n        try:\n            return_value = json.loads(chat_completion.choices[0].message.content)\n            return return_value\n        except:\n            raise ValueError('The response from the AI could not be converted to a json object.')\n    else:\n        chat_completion = client.chat.completions.create(model=model, messages=message_log)\n        return chat_completion.choices[0].message.content\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def read_file(file_path):\n    """Reads and returns the content of a file."""\n    try:\n        logger.info(f\'Reading file {file_path}\')\n        with open(file_path, \'r\') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\'The file {file_path} was not found.\')\n    except IOError:\n        print(f\'Error reading the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def write_file(file_path, content, mode=\'w\'):\n    """Writes content to a file. By default, it overwrites the file.\n    \n    Args:\n    - file_path: Path to the file.\n    - content: Content to be written.\n    - mode: Writing mode (\'w\' for overwrite, \'a\' for append).\n    """\n    try:\n        logger.info(f\'Writing to file {file_path}\')\n        with open(file_path, mode) as file:\n            file.write(content)\n    except IOError:\n        print(f\'Error writing to the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def append_to_file(file_path, content):\n    """Appends content to the end of a file."""\n    write_file(file_path, content, mode=\'a\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def delete_file(file_path):\n    """Deletes a file."""\n    try:\n        logger.info(f\'Deleting file {file_path}\')\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(f\'The file {file_path} does not exist.\')\n    except OSError:\n        print(f\'Error deleting the file {file_path}.\')\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_files_in_directory(directory=\'\'):\n    """Lists all files in a given directory.\n\n    Args:\n    - directory: The path to the directory.\n\n    Returns:\n    - A list of file names in the directory.\n    """\n    try:\n        logger.info(f\'Listing files in directory {directory}\')\n        return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n    except FileNotFoundError:\n        print(f\'Directory {directory} not found.\')\n        return []\n    except OSError as e:\n        print(f\'Error accessing directory {directory}: {e}\')\n        return []\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_current_working_directory():\n    """Returns the current working directory."""\n    logger.info(\'Getting current working directory\')\n    return os.getcwd()\n    \n    Description:'}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def set_env_variables_with_defaults(env_vars_defaults):\n    """\n    Sets environment variables to default values if they are not already set.\n\n    Args:\n    - env_vars_defaults: A dictionary where keys are environment variable names \n                         and values are the default values for these variables.\n    """\n    for var, default in env_vars_defaults.items():\n        if var not in os.environ:\n            if default != \'error\':\n                logger.info(f\'Setting environment variable {var} to default value {default}\')\n                os.environ[var] = default\n            else:\n                logger.error(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n                raise ValueError(f\'Environment variable {var} not set. This is a required variable and no default value provided.\')\n    \n    Description:'}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def return_function_options(functions: [FunctionInfo]) -> str:\n    function_options = ''\n    for function in functions:\n        function_options += f'{function.function_name} : {function.description}\\n'\n    return function_options\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def create_function(function_objective: str, language: str) -> object:\n    prompt = f'Create a function that accomplishes the following: {function_objective}\\n\\n    Using the {language} programming language.\\n\\n    it should return a json object with the following properties:\\n\\n    required_libraries: a list of strings\\n\\n    source_code: a string\\n\\n    The source code should ONLY be the function definition, including the function definition and the function body.\\n    '\n    try:\n        return return_gpt_response(prompt=prompt, return_json_oject=True)\n    except Exception as e:\n        raise ValueError(f'Error creating function: {e}')\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def create_step_list(goal: str) -> object:\n    prompt = f'Create a step list that accomplishes the following: {goal}\\n\\n    The json object returned should have the following properties:\\n    step_list: a list of strings describing the steps to accomplish the goal\\n    verification: a string describing how to verify that the goal has been accomplished\\n    '\n    try:\n        return return_gpt_response(prompt=prompt, return_json_oject=True)\n    except Exception as e:\n        raise ValueError(f'Error creating step list: {e}')\n    \n    Description:"}, {'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def describe_function(function_string):\n    """Returns a description of the function."""\n    prompt = f\'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \\n    \\n    {function_string}\\n    \\n    Description:\'\n    try:\n        logger.info(\'Describing function\')\n        return return_gpt_response(prompt=prompt)\n    except Exception as e:\n        logger.error(f\'Error describing function: {e}\')\n        raise ValueError(\'Error describing function.\')\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 11:53:57,329 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 11:53:57,330 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 11:53:57,331 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 11:53:57,332 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 11:53:57,332 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 11:53:59,242 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 16:53:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1581'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'77779'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1.665s'), (b'x-request-id', b'c5da9590e4bc0f8a4a1bbfa51fecc0ea'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845765e9adde4344-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 11:53:59,251 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 11:53:59,251 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 11:53:59,252 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 11:53:59,252 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 11:53:59,252 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 11:53:59,252 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 11:53:59,255 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 11:53:59,257 - INFO - __main__ - The following 17 function(s) were loaded: ['load_functions_from_file', 'list_functions', 'get_argument_values', 'get_links', 'process_command', 'return_gpt_response', 'read_file', 'write_file', 'append_to_file', 'delete_file', 'list_files_in_directory', 'get_current_working_directory', 'set_env_variables_with_defaults', 'return_function_options', 'create_function', 'create_step_list', 'describe_function']
2024-01-14 11:53:59,391 - DEBUG - httpcore.connection - close.started
2024-01-14 11:53:59,393 - DEBUG - httpcore.connection - close.complete
2024-01-14 12:21:26,692 - INFO - __main__ - Skynet started
2024-01-14 12:21:26,694 - INFO - __main__ - Loading environment variables
2024-01-14 12:21:26,694 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 12:21:26,694 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 12:21:26,694 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 12:21:26,695 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 12:21:26,695 - INFO - __main__ - Loaded functions from functions
2024-01-14 12:21:26,695 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 12:21:26,695 - INFO - functions.get_functions - file size: 2609
2024-01-14 12:21:26,696 - INFO - functions.get_functions - tree size: 16
2024-01-14 12:21:26,908 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 12:21:27,031 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 12:21:27,033 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 12:21:27,033 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 12:21:27,034 - INFO - functions.get_functions - file size: 1926
2024-01-14 12:21:27,035 - INFO - functions.get_functions - tree size: 5
2024-01-14 12:21:27,036 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 12:21:27,038 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 12:21:27,038 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 12:21:27,038 - INFO - functions.get_functions - file size: 2971
2024-01-14 12:21:27,040 - INFO - functions.get_functions - tree size: 10
2024-01-14 12:21:27,040 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 12:21:27,041 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 12:21:27,042 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 12:21:27,043 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 12:21:27,043 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 12:21:27,045 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 12:21:27,045 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 12:21:27,047 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 12:21:27,047 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 12:21:27,047 - INFO - functions.get_functions - file size: 0
2024-01-14 12:21:27,047 - INFO - functions.get_functions - tree size: 0
2024-01-14 12:21:27,048 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 12:21:27,048 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 12:21:27,048 - INFO - functions.get_functions - file size: 2959
2024-01-14 12:21:27,049 - INFO - functions.get_functions - tree size: 9
2024-01-14 12:21:27,050 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 12:21:27,056 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 12:21:27,058 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 12:21:27,060 - INFO - functions.get_functions - Looking at function: required_fields
2024-01-14 12:21:27,064 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 12:21:27,066 - INFO - __main__ - Loaded 5 functions from ./functions/prompt_creation.py
2024-01-14 12:21:27,066 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 12:21:27,066 - INFO - functions.get_functions - file size: 0
2024-01-14 12:21:27,067 - INFO - functions.get_functions - tree size: 0
2024-01-14 12:21:27,069 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 12:21:27,069 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 12:21:27,069 - INFO - functions.get_functions - file size: 5276
2024-01-14 12:21:27,071 - INFO - functions.get_functions - tree size: 9
2024-01-14 12:21:27,074 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 12:21:27,076 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 12:21:27,080 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 12:21:27,083 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 12:21:27,084 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 12:21:27,084 - INFO - functions.prompt_creation - Describing function
2024-01-14 12:21:27,101 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def required_fields(fields: [str], json_object: object) -> bool:\n    """Returns true if the required fields are present in the json object"""\n    for field in fields:\n        if field not in json_object:\n            return False\n    return True\n    \n    Description:'}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-14 12:21:27,222 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-14 12:21:27,316 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3725909150>
2024-01-14 12:21:27,317 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f3725a9e600> server_hostname='api.openai.com' timeout=5.0
2024-01-14 12:21:27,486 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3725e62090>
2024-01-14 12:21:27,487 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 12:21:27,488 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 12:21:27,489 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 12:21:27,489 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 12:21:27,489 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 12:21:28,323 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 17:21:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'482'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79878'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'91ms'), (b'x-request-id', b'9ea43e1db89eafc91daf67f1def65e95'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Q8lgdzZvBJGR.yIWGhf69xk7bLuzmcJKJF1xCL7beY4-1705252888-1-AbyqE63jFBVPKIGje0vm48HLqfNCyHx1lPnACxbKUKKksY0vC0MLTXtnHa9PVerbV0GgLL2vOxzfH8gQ0JIUgN8=; path=/; expires=Sun, 14-Jan-24 17:51:28 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=KBMYJT5T.eMqY1gRaCJ6fUSIJiXeC01iyAfCKwN5Lig-1705252888324-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84578e32f8b541bd-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 12:21:28,324 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 12:21:28,325 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 12:21:28,325 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 12:21:28,326 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 12:21:28,326 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 12:21:28,327 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 12:21:28,330 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 12:21:28,357 - INFO - __main__ - The following 18 function(s) were loaded: ['load_functions_from_file', 'list_functions', 'get_argument_values', 'get_links', 'process_command', 'return_gpt_response', 'read_file', 'write_file', 'append_to_file', 'delete_file', 'list_files_in_directory', 'get_current_working_directory', 'set_env_variables_with_defaults', 'return_function_options', 'create_function', 'create_step_list', 'describe_function', 'required_fields']
2024-01-14 12:21:53,294 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def required_fields(fields: [str], json_object: object) -> bool:\n    """Returns true if the required fields are present in the json object"""\n    for field in fields:\n        if field not in json_object:\n            return False\n    return True\n    \n    Description:'}, {'role': 'user', 'content': 'Create a step list that accomplishes the following:  Please send an email to my co-worker, their email is robert@pretension.io\n\n    The json object returned should have the following properties:\n    step_list: a list of strings describing the steps to accomplish the goal\n    '}], 'model': 'gpt-3.5-turbo-1106', 'response_format': {'type': 'json_object'}}}
2024-01-14 12:21:53,298 - DEBUG - httpcore.connection - close.started
2024-01-14 12:21:53,300 - DEBUG - httpcore.connection - close.complete
2024-01-14 12:21:53,301 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-14 12:21:53,637 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3725c87050>
2024-01-14 12:21:53,638 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f3725a9e600> server_hostname='api.openai.com' timeout=5.0
2024-01-14 12:21:53,803 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f3725127710>
2024-01-14 12:21:53,806 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 12:21:53,807 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 12:21:53,807 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 12:21:53,807 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 12:21:53,807 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 12:22:11,502 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 17:22:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'17505'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79808'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'144ms'), (b'x-request-id', b'b92d16042f2572642de3f28dee33da1b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84578ed77fb96a5c-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 12:22:11,503 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 12:22:11,504 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 12:22:11,504 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 12:22:11,505 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 12:22:11,505 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 12:22:11,505 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 12:22:11,507 - ERROR - __main__ - Error creating step list for  Please send an email to my co-worker, their email is robert@pretension.io
2024-01-14 12:22:11,632 - DEBUG - httpcore.connection - close.started
2024-01-14 12:22:11,633 - DEBUG - httpcore.connection - close.complete
2024-01-14 12:24:50,122 - INFO - __main__ - Skynet started
2024-01-14 12:24:50,123 - INFO - __main__ - Loading environment variables
2024-01-14 12:24:50,123 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 12:24:50,123 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 12:24:50,123 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 12:24:50,124 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 12:24:50,124 - INFO - __main__ - Loaded functions from functions
2024-01-14 12:24:50,125 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 12:24:50,125 - INFO - functions.get_functions - file size: 2609
2024-01-14 12:24:50,126 - INFO - functions.get_functions - tree size: 16
2024-01-14 12:24:50,310 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 12:24:50,434 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 12:24:50,435 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 12:24:50,435 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 12:24:50,435 - INFO - functions.get_functions - file size: 2088
2024-01-14 12:24:50,436 - INFO - functions.get_functions - tree size: 7
2024-01-14 12:24:50,436 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 12:24:50,438 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 12:24:50,438 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 12:24:50,439 - INFO - functions.get_functions - file size: 2971
2024-01-14 12:24:50,440 - INFO - functions.get_functions - tree size: 10
2024-01-14 12:24:50,440 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 12:24:50,441 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 12:24:50,442 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 12:24:50,442 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 12:24:50,443 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 12:24:50,444 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 12:24:50,444 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 12:24:50,446 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 12:24:50,446 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 12:24:50,446 - INFO - functions.get_functions - file size: 0
2024-01-14 12:24:50,447 - INFO - functions.get_functions - tree size: 0
2024-01-14 12:24:50,447 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 12:24:50,447 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 12:24:50,447 - INFO - functions.get_functions - file size: 2959
2024-01-14 12:24:50,449 - INFO - functions.get_functions - tree size: 9
2024-01-14 12:24:50,449 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 12:24:50,450 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 12:24:50,451 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 12:24:50,451 - INFO - functions.get_functions - Looking at function: required_fields
2024-01-14 12:24:50,452 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 12:24:50,458 - INFO - __main__ - Loaded 5 functions from ./functions/prompt_creation.py
2024-01-14 12:24:50,458 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 12:24:50,459 - INFO - functions.get_functions - file size: 0
2024-01-14 12:24:50,459 - INFO - functions.get_functions - tree size: 0
2024-01-14 12:24:50,459 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 12:24:50,459 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 12:24:50,459 - INFO - functions.get_functions - file size: 5276
2024-01-14 12:24:50,461 - INFO - functions.get_functions - tree size: 9
2024-01-14 12:24:50,461 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 12:24:50,464 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 12:24:50,465 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 12:24:50,467 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 12:24:50,468 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 12:24:50,469 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 12:24:50,471 - INFO - __main__ - The following 18 function(s) were loaded: ['load_functions_from_file', 'list_functions', 'get_argument_values', 'get_links', 'process_command', 'return_gpt_response', 'read_file', 'write_file', 'append_to_file', 'delete_file', 'list_files_in_directory', 'get_current_working_directory', 'set_env_variables_with_defaults', 'return_function_options', 'create_function', 'create_step_list', 'describe_function', 'required_fields']
2024-01-14 12:25:05,198 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Create a step list that accomplishes the following: Please send an email to my co-worker whose email is robert@pretension.io\n\n    The json object returned should have the following properties:\n    step_list: a list of strings describing the steps to accomplish the goal\n    '}], 'model': 'gpt-3.5-turbo-1106', 'response_format': {'type': 'json_object'}}}
2024-01-14 12:25:05,249 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-14 12:25:05,497 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9d284f60d0>
2024-01-14 12:25:05,498 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9d28e5a570> server_hostname='api.openai.com' timeout=5.0
2024-01-14 12:25:05,732 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9d284dec10>
2024-01-14 12:25:05,733 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 12:25:05,734 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 12:25:05,734 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 12:25:05,735 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 12:25:05,735 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 12:25:07,810 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 17:25:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1493'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79914'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'64ms'), (b'x-request-id', b'43f469650f147ee0d9004ec89feee8a0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=86NCd6L6uFmTVi8fc_fSeoRVgHKzyqIAf7wbVqB8j9U-1705253107-1-AetFQ17mB+7lfk5yGh3g1o0r7IbhUwcQ0Y8G467whhQ3R56E9+kYKwB4rwyLZtCLYJEvl0YR90KjjTiCDe+dFvo=; path=/; expires=Sun, 14-Jan-24 17:55:07 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=nT.hGlUH1YsBTiV4kIOwfySpGAeEhp_Ax_v1SSMxCKE-1705253107573-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84579386f9f019e3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 12:25:07,812 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 12:25:07,840 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 12:25:07,842 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 12:25:07,843 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 12:25:07,843 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 12:25:07,844 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 12:25:07,853 - INFO - functions.openai_call - Converting response        {
    "step_list": [
      "Open your email client or email service provider",
      "Click on the 'Compose' or 'New Message' button",
      "In the 'To' field, type in 'robert@pretension.io'",
      "Write your message in the body of the email",
      "Check for any attachments or links you want to include",
      "Proofread your email for any errors",
      "Click 'Send' to send the email to robert@pretension.io"
    ]
  } to json object
2024-01-14 12:28:01,072 - DEBUG - httpcore.connection - close.started
2024-01-14 12:28:01,073 - DEBUG - httpcore.connection - close.complete
2024-01-14 12:28:03,075 - INFO - __main__ - Skynet started
2024-01-14 12:28:03,076 - INFO - __main__ - Loading environment variables
2024-01-14 12:28:03,076 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-14 12:28:03,076 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-14 12:28:03,077 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-14 12:28:03,077 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-14 12:28:03,077 - INFO - __main__ - Loaded functions from functions
2024-01-14 12:28:03,077 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-14 12:28:03,078 - INFO - functions.get_functions - file size: 2609
2024-01-14 12:28:03,079 - INFO - functions.get_functions - tree size: 16
2024-01-14 12:28:03,219 - INFO - functions.get_functions - Looking at function: get_links
2024-01-14 12:28:03,317 - INFO - functions.get_functions - Looking at function: process_command
2024-01-14 12:28:03,318 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-14 12:28:03,318 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-14 12:28:03,318 - INFO - functions.get_functions - file size: 2144
2024-01-14 12:28:03,319 - INFO - functions.get_functions - tree size: 7
2024-01-14 12:28:03,319 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-14 12:28:03,321 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-14 12:28:03,321 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-14 12:28:03,321 - INFO - functions.get_functions - file size: 2971
2024-01-14 12:28:03,322 - INFO - functions.get_functions - tree size: 10
2024-01-14 12:28:03,323 - INFO - functions.get_functions - Looking at function: read_file
2024-01-14 12:28:03,323 - INFO - functions.get_functions - Looking at function: write_file
2024-01-14 12:28:03,326 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-14 12:28:03,327 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-14 12:28:03,328 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-14 12:28:03,329 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-14 12:28:03,330 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-14 12:28:03,331 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-14 12:28:03,332 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-14 12:28:03,332 - INFO - functions.get_functions - file size: 0
2024-01-14 12:28:03,333 - INFO - functions.get_functions - tree size: 0
2024-01-14 12:28:03,333 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-14 12:28:03,333 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-14 12:28:03,334 - INFO - functions.get_functions - file size: 3155
2024-01-14 12:28:03,335 - INFO - functions.get_functions - tree size: 9
2024-01-14 12:28:03,336 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-14 12:28:03,337 - INFO - functions.get_functions - Looking at function: create_function
2024-01-14 12:28:03,341 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-14 12:28:03,343 - INFO - functions.get_functions - Looking at function: required_fields
2024-01-14 12:28:03,344 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-14 12:28:03,345 - INFO - __main__ - Loaded 5 functions from ./functions/prompt_creation.py
2024-01-14 12:28:03,346 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-14 12:28:03,346 - INFO - functions.get_functions - file size: 0
2024-01-14 12:28:03,346 - INFO - functions.get_functions - tree size: 0
2024-01-14 12:28:03,347 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-14 12:28:03,347 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-14 12:28:03,347 - INFO - functions.get_functions - file size: 5276
2024-01-14 12:28:03,349 - INFO - functions.get_functions - tree size: 9
2024-01-14 12:28:03,350 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-14 12:28:03,355 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-14 12:28:03,356 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-14 12:28:03,360 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-14 12:28:03,360 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-14 12:28:03,361 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-14 12:28:03,363 - INFO - __main__ - The following 18 function(s) were loaded: ['load_functions_from_file', 'list_functions', 'get_argument_values', 'get_links', 'process_command', 'return_gpt_response', 'read_file', 'write_file', 'append_to_file', 'delete_file', 'list_files_in_directory', 'get_current_working_directory', 'set_env_variables_with_defaults', 'return_function_options', 'create_function', 'create_step_list', 'describe_function', 'required_fields']
2024-01-14 12:28:17,385 - INFO - functions.openai_call - 
Sending Prompt:
Create a step list that accomplishes the following: Please send an email to my co-worker whose email is: robert@pretension.io

    Note that you can only use the command line on the ubuntu operating system, and you can only use the following python functions:

    load_functions_from_file : This function loads Python function definitions from a given file path. It reads the file, analyses its contents, and returns a list of information about each function, including the function name, source code, and argument types. If it encounters errors such as syntax errors or file-related issues, it logs an error message and returns an empty list.
list_functions : The 'list_functions' function logs the available functions and their argument types from the input 'functions' dictionary. It outputs these details as informational log messages.
get_argument_values : This function takes argument types and prompts the user to enter values for each argument. The input is then converted to the corresponding type and added to a list. If the input is invalid, errors are logged. The function returns the list of argument values.
get_links : The function takes a URL as input, sends a request to the URL, and then scrapes the HTML content to find and return a list of all the links (URLs) found on the page.
process_command : The function processes a given command. If the command is 'get_links', it calls the get_links function with the input provided after 'get_links'. If the command is 'exit', it returns None. If the command is unknown, it returns 'Unknown command'. If there is a value error during 'get_links', it returns 'Error: Please provide valid numbers'.
return_gpt_response : The function `return_gpt_response` takes in input variables `message_log`, `prompt`, `model`, and `return_json_object`. It checks for empty inputs and then processes user chat input using OpenAI's GPT model to generate a response. If `return_json_object` is set to `True`, it returns a JSON object of the response; otherwise, it returns the generated chat response. If model type is not provided, it uses the default GPT model from the environment variables.
read_file : The function "read_file" takes a file path as input and returns the content of the file. It logs an info message for the file being read, and then attempts to read and return the file's contents. If the file is not found, it prints a message stating the file was not found. If there is an error reading the file, it prints a message indicating an error occurred.
write_file : The function "write_file" takes three input variables - "file_path", "content", and "mode" (with a default value of 'w'). It writes the given content to the file specified by "file_path" in the specified mode (either overwrite or append). If the file writing operation encounters an error, an appropriate error message is displayed.
append_to_file : The function "append_to_file" appends content to the end of a file by calling the "write_file" function with the append mode.
delete_file : The function "delete_file" deletes the specified file. It logs the file deletion, and if the file doesn't exist, it prints a message that the file does not exist. If there is any error during the file deletion, it prints an error message indicating an issue with the file deletion.
list_files_in_directory : This function lists all files in a given directory path. If the directory path exists, it returns a list of file names within that directory. If the directory does not exist or an error occurs during access, it returns an empty list with a corresponding error message.
get_current_working_directory : The "get_current_working_directory" function returns the path to the current working directory of the program. If the directory cannot be accessed or encountered during the process, the function logs an error and returns the failure message.
set_env_variables_with_defaults : This function sets environment variables to default values if they are not already set. It takes in a dictionary where keys are environment variable names and values are the default values for these variables. If the environment variable is not set, it is assigned the default value. If a required variable doesn't have a default value, it raises an error.
return_function_options : This function takes a list of FunctionInfo objects as input and returns a string containing the name and description of each function in the list. Each function's name and description is concatenated together into the output string, separated by a colon and a new line.
create_function : The `create_function` function generates a prompt for creating a function with a specified objective and programming language. It then uses the GPT (Generative Pre-trained Transformer) model to generate the source code for the function as a JSON object, including required libraries and the source code. If an error occurs during the function creation process, it raises an exception with a corresponding error message.
create_step_list : The function "create_step_list" takes in a goal and generates a step list in the form of a JSON object. It prompts the user with the goal and returns a structured step list as a JSON object containing a list of strings outlining the steps needed to achieve the goal, as well as a string describing how to verify the completion of the goal. If any errors occur during the process, it raises a value error along with the error message.
describe_function : The "describe_function" function takes a function string as input and returns a description of the function by using GPT-3 natural language processing. It creates a prompt matching the format of a succinct description for a function, then uses GPT-3 to generate the description based on the provided function string. If there are any errors during the process, it raises a ValueError.
required_fields : The function checks if all the required fields specified in the list are present in the given JSON object. If all the required fields are present, it returns `True`; otherwise, it returns `False`.


    The json object returned should have the following properties:
    step_list: a list of strings describing the steps to accomplish the goal
    


2024-01-14 12:28:17,390 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Create a step list that accomplishes the following: Please send an email to my co-worker whose email is: robert@pretension.io\n\n    Note that you can only use the command line on the ubuntu operating system, and you can only use the following python functions:\n\n    load_functions_from_file : This function loads Python function definitions from a given file path. It reads the file, analyses its contents, and returns a list of information about each function, including the function name, source code, and argument types. If it encounters errors such as syntax errors or file-related issues, it logs an error message and returns an empty list.\nlist_functions : The \'list_functions\' function logs the available functions and their argument types from the input \'functions\' dictionary. It outputs these details as informational log messages.\nget_argument_values : This function takes argument types and prompts the user to enter values for each argument. The input is then converted to the corresponding type and added to a list. If the input is invalid, errors are logged. The function returns the list of argument values.\nget_links : The function takes a URL as input, sends a request to the URL, and then scrapes the HTML content to find and return a list of all the links (URLs) found on the page.\nprocess_command : The function processes a given command. If the command is \'get_links\', it calls the get_links function with the input provided after \'get_links\'. If the command is \'exit\', it returns None. If the command is unknown, it returns \'Unknown command\'. If there is a value error during \'get_links\', it returns \'Error: Please provide valid numbers\'.\nreturn_gpt_response : The function `return_gpt_response` takes in input variables `message_log`, `prompt`, `model`, and `return_json_object`. It checks for empty inputs and then processes user chat input using OpenAI\'s GPT model to generate a response. If `return_json_object` is set to `True`, it returns a JSON object of the response; otherwise, it returns the generated chat response. If model type is not provided, it uses the default GPT model from the environment variables.\nread_file : The function "read_file" takes a file path as input and returns the content of the file. It logs an info message for the file being read, and then attempts to read and return the file\'s contents. If the file is not found, it prints a message stating the file was not found. If there is an error reading the file, it prints a message indicating an error occurred.\nwrite_file : The function "write_file" takes three input variables - "file_path", "content", and "mode" (with a default value of \'w\'). It writes the given content to the file specified by "file_path" in the specified mode (either overwrite or append). If the file writing operation encounters an error, an appropriate error message is displayed.\nappend_to_file : The function "append_to_file" appends content to the end of a file by calling the "write_file" function with the append mode.\ndelete_file : The function "delete_file" deletes the specified file. It logs the file deletion, and if the file doesn\'t exist, it prints a message that the file does not exist. If there is any error during the file deletion, it prints an error message indicating an issue with the file deletion.\nlist_files_in_directory : This function lists all files in a given directory path. If the directory path exists, it returns a list of file names within that directory. If the directory does not exist or an error occurs during access, it returns an empty list with a corresponding error message.\nget_current_working_directory : The "get_current_working_directory" function returns the path to the current working directory of the program. If the directory cannot be accessed or encountered during the process, the function logs an error and returns the failure message.\nset_env_variables_with_defaults : This function sets environment variables to default values if they are not already set. It takes in a dictionary where keys are environment variable names and values are the default values for these variables. If the environment variable is not set, it is assigned the default value. If a required variable doesn\'t have a default value, it raises an error.\nreturn_function_options : This function takes a list of FunctionInfo objects as input and returns a string containing the name and description of each function in the list. Each function\'s name and description is concatenated together into the output string, separated by a colon and a new line.\ncreate_function : The `create_function` function generates a prompt for creating a function with a specified objective and programming language. It then uses the GPT (Generative Pre-trained Transformer) model to generate the source code for the function as a JSON object, including required libraries and the source code. If an error occurs during the function creation process, it raises an exception with a corresponding error message.\ncreate_step_list : The function "create_step_list" takes in a goal and generates a step list in the form of a JSON object. It prompts the user with the goal and returns a structured step list as a JSON object containing a list of strings outlining the steps needed to achieve the goal, as well as a string describing how to verify the completion of the goal. If any errors occur during the process, it raises a value error along with the error message.\ndescribe_function : The "describe_function" function takes a function string as input and returns a description of the function by using GPT-3 natural language processing. It creates a prompt matching the format of a succinct description for a function, then uses GPT-3 to generate the description based on the provided function string. If there are any errors during the process, it raises a ValueError.\nrequired_fields : The function checks if all the required fields specified in the list are present in the given JSON object. If all the required fields are present, it returns `True`; otherwise, it returns `False`.\n\n\n    The json object returned should have the following properties:\n    step_list: a list of strings describing the steps to accomplish the goal\n    '}], 'model': 'gpt-3.5-turbo-1106', 'response_format': {'type': 'json_object'}}}
2024-01-14 12:28:17,439 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-14 12:28:17,489 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f091bb43c90>
2024-01-14 12:28:17,489 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f091c4ae570> server_hostname='api.openai.com' timeout=5.0
2024-01-14 12:28:17,596 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f091bb438d0>
2024-01-14 12:28:17,597 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-14 12:28:17,598 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-14 12:28:17,599 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-14 12:28:17,599 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-14 12:28:17,599 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-14 12:28:21,877 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 14 Jan 2024 17:28:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'3370'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78428'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1.179s'), (b'x-request-id', b'02cda5ec1723fa60bb7526af59059783'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=o74qEsYPe9APGIzWX8h2QlBjSqkJrRvOe04eK6GxU6U-1705253301-1-AZwv254ho2zv+i73nE5ukLSV83IOpkhn+GdhyyHGa0nS7K8cW4c8aNZqvR0Q8aZk9PdmnwDaivMXSN5U7v3SPdM=; path=/; expires=Sun, 14-Jan-24 17:58:21 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=g66JYc1oR4MkYbqhsroDmPzXsbDqmGNQ2_0Udj8n1UE-1705253301836-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'845798361ac91a0f-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-14 12:28:21,881 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-14 12:28:21,882 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-14 12:28:21,883 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-14 12:28:21,883 - DEBUG - httpcore.http11 - response_closed.started
2024-01-14 12:28:21,884 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-14 12:28:21,885 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-14 12:28:21,889 - INFO - functions.openai_call - Converting response 
   {
      "goal": "Send an email to the co-worker",
      "step_list": [
        "Open the terminal on the Ubuntu operating system",
        "Use the 'get_current_working_directory' function to ensure you are in the correct directory",
        "If not in the correct directory, use the 'list_files_in_directory' function to list the files in the current directory and navigate to the appropriate location",
        "Use the 'load_functions_from_file' function to load the necessary Python functions",
        "Create a list of required fields for the email such as recipient, subject, and body",
        "Prompt the user to enter the recipient, subject, and body using the 'get_argument_values' function",
        "Use the 'return_gpt_response' function to generate the email content based on the provided inputs",
        "Verify the generated email content",
        "Use the 'write_file' function to write the generated email content to a file",
        "Use the 'send_email' function to send the email to robert@pretension.io",
        "Verify that the email has been sent successfully"
      ]
    }
    
     to json object
2024-01-14 12:29:43,013 - DEBUG - httpcore.connection - close.started
2024-01-14 12:29:43,014 - DEBUG - httpcore.connection - close.complete
