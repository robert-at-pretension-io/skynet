2024-01-13 14:29:56,541 - INFO - __main__ - Skynet started
2024-01-13 14:29:56,541 - INFO - __main__ - Loading environment variables
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Setting environment variable DEFAULT_GPT_MODEL to default value gpt-3.5-turbo-1106
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Setting environment variable FUNCTIONS_FOLDER to default value functions
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Setting environment variable FUNCTION_INFO_SERIALIZATION_FILES to default value serialized_function_info.json
2024-01-13 14:29:56,541 - INFO - functions.file_system_primitives - Listing files in directory functions
2024-01-13 14:29:56,541 - INFO - __main__ - Loaded functions from functions
2024-01-13 14:29:56,542 - INFO - __main__ - Trying to load functions from ./functions/web_browsing.py
2024-01-13 14:29:56,542 - INFO - functions.get_functions - file size: 2609
2024-01-13 14:29:56,543 - INFO - functions.get_functions - tree size: 16
2024-01-13 14:29:56,689 - INFO - functions.get_functions - Looking at function: get_links
2024-01-13 14:29:56,790 - INFO - functions.get_functions - Looking at function: process_command
2024-01-13 14:29:56,792 - INFO - __main__ - Loaded 2 functions from ./functions/web_browsing.py
2024-01-13 14:29:56,792 - INFO - __main__ - Trying to load functions from ./functions/openai_call.py
2024-01-13 14:29:56,793 - INFO - functions.get_functions - file size: 1926
2024-01-13 14:29:56,793 - INFO - functions.get_functions - tree size: 5
2024-01-13 14:29:56,794 - INFO - functions.get_functions - Looking at function: return_gpt_response
2024-01-13 14:29:56,795 - INFO - __main__ - Loaded 1 functions from ./functions/openai_call.py
2024-01-13 14:29:56,795 - INFO - __main__ - Trying to load functions from ./functions/file_system_primitives.py
2024-01-13 14:29:56,796 - INFO - functions.get_functions - file size: 2971
2024-01-13 14:29:56,797 - INFO - functions.get_functions - tree size: 10
2024-01-13 14:29:56,803 - INFO - functions.get_functions - Looking at function: read_file
2024-01-13 14:29:56,808 - INFO - functions.get_functions - Looking at function: write_file
2024-01-13 14:29:56,809 - INFO - functions.get_functions - Looking at function: append_to_file
2024-01-13 14:29:56,810 - INFO - functions.get_functions - Looking at function: delete_file
2024-01-13 14:29:56,811 - INFO - functions.get_functions - Looking at function: list_files_in_directory
2024-01-13 14:29:56,816 - INFO - functions.get_functions - Looking at function: get_current_working_directory
2024-01-13 14:29:56,816 - INFO - functions.get_functions - Looking at function: set_env_variables_with_defaults
2024-01-13 14:29:56,818 - INFO - __main__ - Loaded 7 functions from ./functions/file_system_primitives.py
2024-01-13 14:29:56,824 - INFO - __main__ - Trying to load functions from ./functions/__init__.py
2024-01-13 14:29:56,825 - INFO - functions.get_functions - file size: 0
2024-01-13 14:29:56,825 - INFO - functions.get_functions - tree size: 0
2024-01-13 14:29:56,825 - INFO - __main__ - Loaded 0 functions from ./functions/__init__.py
2024-01-13 14:29:56,826 - INFO - __main__ - Trying to load functions from ./functions/prompt_creation.py
2024-01-13 14:29:56,826 - INFO - functions.get_functions - file size: 2135
2024-01-13 14:29:56,827 - INFO - functions.get_functions - tree size: 8
2024-01-13 14:29:56,827 - INFO - functions.get_functions - Looking at function: return_function_options
2024-01-13 14:29:56,828 - INFO - functions.get_functions - Looking at function: create_function
2024-01-13 14:29:56,829 - INFO - functions.get_functions - Looking at function: create_step_list
2024-01-13 14:29:56,830 - INFO - functions.get_functions - Looking at function: describe_function
2024-01-13 14:29:56,832 - INFO - __main__ - Loaded 4 functions from ./functions/prompt_creation.py
2024-01-13 14:29:56,840 - INFO - __main__ - Trying to load functions from ./functions/system_loop_primatives.py
2024-01-13 14:29:56,841 - INFO - functions.get_functions - file size: 0
2024-01-13 14:29:56,841 - INFO - functions.get_functions - tree size: 0
2024-01-13 14:29:56,841 - INFO - __main__ - Loaded 0 functions from ./functions/system_loop_primatives.py
2024-01-13 14:29:56,843 - INFO - __main__ - Trying to load functions from ./functions/get_functions.py
2024-01-13 14:29:56,846 - INFO - functions.get_functions - file size: 5276
2024-01-13 14:29:56,850 - INFO - functions.get_functions - tree size: 9
2024-01-13 14:29:56,854 - INFO - functions.get_functions - Looking at function: load_functions_from_file
2024-01-13 14:29:56,857 - INFO - functions.get_functions - Looking at function: list_functions
2024-01-13 14:29:56,861 - INFO - functions.get_functions - Looking at function: get_argument_values
2024-01-13 14:29:56,866 - INFO - __main__ - Loaded 3 functions from ./functions/get_functions.py
2024-01-13 14:29:56,872 - INFO - system_objects.functions - Loading function infos from serialized_function_info.json
2024-01-13 14:29:56,873 - INFO - functions.prompt_creation - Describing function
2024-01-13 14:29:56,879 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-13 14:29:56,917 - DEBUG - httpcore.connection - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-13 14:29:57,053 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f8f9d3b2390>
2024-01-13 14:29:57,053 - DEBUG - httpcore.connection - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f8f9dd06600> server_hostname='api.openai.com' timeout=5.0
2024-01-13 14:29:57,117 - DEBUG - httpcore.connection - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f8f9db7ecd0>
2024-01-13 14:29:57,118 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-13 14:29:57,118 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-13 14:29:57,118 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-13 14:29:57,119 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-13 14:29:57,119 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-13 14:29:59,062 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 13 Jan 2024 19:29:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1832'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79370'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'472ms'), (b'x-request-id', b'7fa1197b44352f0c7b3406b7f62617d0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=IXURbJxAgM8SVdq6AuN2P5xzRW8pph50VAcI_7DPaec-1705174199-1-AQ6eEn3+TnBBa7Bb0KRmFIRySzJflux59DpiHYpWqdCgHJC8OxXDuTwhcVbNf8jnvLtqqF39WVs4yxY2jzwVMOg=; path=/; expires=Sat, 13-Jan-24 19:59:59 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=pKqUAUfvVxrme34W1Y2v6q9TQ.eRooValIW8LL7mqPY-1705174199041-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84500d0c2eb97bb1-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-13 14:29:59,063 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-13 14:29:59,064 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-13 14:29:59,065 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-13 14:29:59,065 - DEBUG - httpcore.http11 - response_closed.started
2024-01-13 14:29:59,065 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-13 14:29:59,066 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-13 14:29:59,069 - INFO - functions.prompt_creation - Describing function
2024-01-13 14:29:59,080 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_functions(functions):\n    logger.info('Available functions:')\n    for func_name, func_info in functions.items():\n        arg_types = func_info['arg_types']\n        logger.info(f'Function: {func_name}, Argument Types: {arg_types}')\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-13 14:29:59,082 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-13 14:29:59,083 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-13 14:29:59,083 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-13 14:29:59,084 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-13 14:29:59,084 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-13 14:30:01,213 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 13 Jan 2024 19:30:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1987'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'79266'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'550ms'), (b'x-request-id', b'871161f984fbfa4b6370f8f7ae47ff72'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84500d18684d7bb1-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-13 14:30:01,214 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-13 14:30:01,215 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-13 14:30:01,215 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-13 14:30:01,216 - DEBUG - httpcore.http11 - response_closed.started
2024-01-13 14:30:01,216 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-13 14:30:01,216 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-13 14:30:01,218 - INFO - functions.prompt_creation - Describing function
2024-01-13 14:30:01,229 - DEBUG - openai._base_client - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def load_functions_from_file(file_path) -> [FunctionInfo]:\n    try:\n        with open(file_path, 'r') as file:\n            file_contents = file.read()\n            logger.info(f'file size: {len(file_contents)}')\n            try:\n                tree = ast.parse(file_contents, type_comments=True)\n                logger.info(f'tree size: {len(tree.body)}')\n                functions = []\n                imported_modules = {}\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for name in node.names:\n                            imported_modules[name.name] = importlib.import_module(name.name)\n                    elif isinstance(node, ast.ImportFrom):\n                        module = importlib.import_module(node.module)\n                        for name in node.names:\n                            imported_modules[name.name] = getattr(module, name.name)\n                    if isinstance(node, ast.FunctionDef):\n                        logger.info(f'Looking at function: {node.name}')\n                        func_name = node.name\n                        func_code = compile(ast.Module(body=[node], type_ignores=[]), filename='<ast>', mode='exec')\n                        temp_namespace = {**imported_modules}\n                        exec(func_code, temp_namespace)\n                        func = temp_namespace[func_name]\n                        sig = inspect.signature(func)\n                        arg_types = {param_name: param.annotation for param_name, param in sig.parameters.items()}\n                        source_code = ast.unparse(node)\n                        function_info = FunctionInfo(func_name, file_path, source_code, '')\n                        functions.append(function_info)\n                return functions\n            except SyntaxError as e:\n                logger.error(f'Syntax error in {file_path}: {e}')\n                return []\n            except Exception as e:\n                logger.error(f'Error loading functions from {file_path}: {e}')\n                return []\n    except FileNotFoundError:\n        logger.error(f'The file {file_path} was not found.')\n        return []\n    except IOError:\n        logger.error(f'Error reading the file {file_path}.')\n        return []\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def list_functions(functions):\n    logger.info('Available functions:')\n    for func_name, func_info in functions.items():\n        arg_types = func_info['arg_types']\n        logger.info(f'Function: {func_name}, Argument Types: {arg_types}')\n    \n    Description:"}, {'role': 'user', 'content': "Describe the function, using the input variables. Make the description succinct though covering its entire functionality using plain english: \n    \n    def get_argument_values(arg_types):\n    args = []\n    for arg_name, arg_type in arg_types.items():\n        while True:\n            user_input = input(f'Enter value for {arg_name} ({arg_type}): ')\n            try:\n                if arg_type == int:\n                    converted_value = int(user_input)\n                elif arg_type == float:\n                    converted_value = float(user_input)\n                elif arg_type == bool:\n                    converted_value = user_input.lower() in ['true', '1', 'yes']\n                elif arg_type == str:\n                    converted_value = user_input\n                else:\n                    converted_value = eval(user_input)\n                if not isinstance(converted_value, arg_type) and arg_type != inspect._empty:\n                    raise TypeError(f'Incorrect type for {arg_name}, expected {arg_type}')\n                break\n            except ValueError as e:\n                logger.error(f'Invalid input: {e}')\n            except TypeError as e:\n                logger.error(e)\n            except Exception as e:\n                logger.error(f'Error processing input: {e}')\n        args.append(converted_value)\n    return args\n    \n    Description:"}], 'model': 'gpt-3.5-turbo-1106'}}
2024-01-13 14:30:01,230 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - send_request_headers.complete
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - send_request_body.complete
2024-01-13 14:30:01,231 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2024-01-13 14:30:02,748 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 13 Jan 2024 19:30:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-1106'), (b'openai-organization', b'user-codnuftfktazlnfurmq8wq2u'), (b'openai-processing-ms', b'1174'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'80000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'78923'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'807ms'), (b'x-request-id', b'f21e679a51c0ea669e3063e7cb2f2599'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84500d25da217bb1-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-13 14:30:02,749 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-01-13 14:30:02,749 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2024-01-13 14:30:02,751 - DEBUG - httpcore.http11 - receive_response_body.complete
2024-01-13 14:30:02,751 - DEBUG - httpcore.http11 - response_closed.started
2024-01-13 14:30:02,751 - DEBUG - httpcore.http11 - response_closed.complete
2024-01-13 14:30:02,751 - DEBUG - openai._base_client - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-01-13 14:30:02,754 - INFO - system_objects.functions - Saving function infos to serialized_function_info.json
2024-01-13 14:30:02,903 - DEBUG - httpcore.connection - close.started
2024-01-13 14:30:02,904 - DEBUG - httpcore.connection - close.complete
